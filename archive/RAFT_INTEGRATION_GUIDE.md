# Raft Consensus Integration Guide

## Overview

This document explains the Raft consensus integration in the Distributed ZKP Network coordinator to eliminate single points of failure and enable high availability through leader election and state replication.

---

## Table of Contents

1. [What is Raft?](#what-is-raft)
2. [Why Raft for Coordinators?](#why-raft-for-coordinators)
3. [Architecture](#architecture)
4. [Component Changes](#component-changes)
5. [Configuration](#configuration)
6. [Deployment](#deployment)
7. [Operation Visualization](#operation-visualization)
8. [Troubleshooting](#troubleshooting)

---

## What is Raft?

Raft is a consensus algorithm designed for managing a replicated log. It provides:

- **Leader Election**: Automatic selection of a leader node
- **Log Replication**: Consistent state across all nodes
- **Safety**: Ensures committed entries are never lost
- **Membership Changes**: Dynamic cluster reconfiguration

### Key Concepts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Raft Consensus                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ FOLLOWER â”‚      â”‚  LEADER  â”‚      â”‚ FOLLOWER â”‚         â”‚
â”‚  â”‚  Node 1  â”‚â—„â”€â”€â”€â”€â”€â”¤  Node 2  â”œâ”€â”€â”€â”€â”€â–ºâ”‚  Node 3  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚                  â”‚                  â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              Replicated Log Entries                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Node States**:

- **Leader**: Handles all client requests, replicates logs to followers
- **Follower**: Replicates logs from leader, can become candidate
- **Candidate**: Requests votes during election

---

## Why Raft for Coordinators?

### Problem: Single Point of Failure

**Before Raft**:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Coordinator  â”‚
                    â”‚  (Single)    â”‚â—„â”€â”€â”€â”€ ALL workers connect here
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   âŒ FAILURE = System Down
```

### Solution: Highly Available Coordinator Cluster

**After Raft**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Coordinator Cluster (3 nodes)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Coordinator-1 â”‚  â”‚Coordinator-2 â”‚  â”‚Coordinator-3 â”‚     â”‚
â”‚  â”‚  (Follower)  â”‚  â”‚   (LEADER)   â”‚  â”‚  (Follower)  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                  Raft Consensus                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                     â”‚
           â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
           â”‚ Worker-1 â”‚         â”‚ Worker-2 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits

1. **High Availability**: If leader fails, new leader elected automatically
2. **Consistency**: All coordinators have same task assignment state
3. **Scalability**: Add more coordinators without downtime
4. **Fault Tolerance**: Tolerates (N-1)/2 failures (e.g., 3-node cluster tolerates 1 failure)

---

## Architecture

### Components

#### 1. RaftNode (`raft_node.go`)

**Purpose**: Manages Raft consensus protocol

**Responsibilities**:

- Leader election
- Log replication
- Cluster membership
- Snapshot management

```go
type RaftNode struct {
    raft   *raft.Raft          // HashiCorp Raft instance
    fsm    *CoordinatorFSM     // Finite State Machine
    logger *zap.Logger
}
```

**Key Methods**:

```go
IsLeader() bool                  // Check if this node is the leader
GetState() raft.RaftState        // Get current state (Leader/Follower/Candidate)
GetLeaderAddress() string        // Get current leader address
Apply(cmd []byte) error          // Replicate command to cluster
Bootstrap(peers []Server) error  // Initialize cluster
Shutdown() error                 // Gracefully stop node
```

#### 2. CoordinatorFSM (`fsm.go`)

**Purpose**: Finite State Machine for applying Raft log entries

**Responsibilities**:

- Apply committed log entries to coordinator state
- Create snapshots for recovery
- Restore from snapshots

```go
type CoordinatorFSM struct {
    workerRegistry *registry.WorkerRegistry
    taskRepo       postgres.TaskRepository
    logger         *zap.Logger
}
```

**Command Types**:

```go
const (
    CmdTaskAssigned  CommandType = "task_assigned"   // Task assigned to worker
    CmdTaskCompleted CommandType = "task_completed"  // Task completed by worker
    CmdWorkerAdded   CommandType = "worker_added"    // Worker joined cluster
    CmdWorkerRemoved CommandType = "worker_removed"  // Worker left cluster
)
```

**Flow**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FSM Apply Cycle                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Leader receives client request                         â”‚
â”‚     (e.g., assign task to worker)                          â”‚
â”‚                   â”‚                                         â”‚
â”‚                   â–¼                                         â”‚
â”‚  2. Leader creates Command                                 â”‚
â”‚     {type: "task_assigned", payload: {...}}                â”‚
â”‚                   â”‚                                         â”‚
â”‚                   â–¼                                         â”‚
â”‚  3. Leader replicates to followers                         â”‚
â”‚     via Raft.Apply()                                       â”‚
â”‚                   â”‚                                         â”‚
â”‚                   â–¼                                         â”‚
â”‚  4. Once majority ack, committed                           â”‚
â”‚                   â”‚                                         â”‚
â”‚                   â–¼                                         â”‚
â”‚  5. FSM.Apply() called on ALL nodes                        â”‚
â”‚     - Leader: updates local state                          â”‚
â”‚     - Followers: replicate state change                    â”‚
â”‚                   â”‚                                         â”‚
â”‚                   â–¼                                         â”‚
â”‚  6. State now consistent across cluster                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. Modified Scheduler (`task_scheduler.go`)

**Key Change**: Only leader schedules tasks

```go
func (ts *TaskScheduler) schedulingLoop() {
    ticker := time.NewTicker(ts.pollInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            // âš ï¸ CRITICAL: Only leader schedules tasks
            if !ts.raftNode.IsLeader() {
                ts.logger.Debug("Skipping schedule cycle (not leader)")
                continue
            }

            ts.metrics.PollCycles++
            if err := ts.scheduleOneCycle(); err != nil {
                ts.logger.Error("Scheduling cycle failed", zap.Error(err))
            }

        case <-ts.ctx.Done():
            return
        }
    }
}
```

**Why?** Prevents duplicate task assignments if multiple coordinators run simultaneously.

#### 4. Modified gRPC Service (`grpc_service.go`)

**Key Changes**: Leader check for worker operations

```go
func (s *CoordinatorGRPCService) RegisterWorker(
    ctx context.Context,
    req *pb.RegisterWorkerRequest,
) (*pb.RegisterWorkerResponse, error) {
    // Redirect to leader if not leader
    if !s.raftNode.IsLeader() {
        leaderAddr := s.raftNode.GetLeaderAddress()
        return &pb.RegisterWorkerResponse{
            Success: false,
            Message: fmt.Sprintf("Not leader, connect to: %s", leaderAddr),
        }, nil
    }

    // Leader handles registration...
}
```

**Why?** Ensures all state-changing operations go through leader for consistency.

---

## Component Changes

### Summary of Changes

| Component        | File                                               | Changes                                                   |
| ---------------- | -------------------------------------------------- | --------------------------------------------------------- |
| Coordinator Main | `cmd/coordinator/main.go`                          | Added Raft initialization (STEP 6), updated shutdown      |
| Config           | `internal/common/config/config.go`                 | Added `RaftConfig` and `RaftPeer` structs                 |
| Scheduler        | `internal/coordinator/scheduler/task_scheduler.go` | Added `raftNode` field, leader check in scheduling loop   |
| gRPC Service     | `internal/coordinator/service/grpc_service.go`     | Added `raftNode` field, leader checks in write operations |
| Raft Node        | `internal/coordinator/raft/raft_node.go`           | **NEW** - Raft node wrapper                               |
| FSM              | `internal/coordinator/raft/fsm.go`                 | **NEW** - State machine implementation                    |

### Detailed Changes

#### coordinator/main.go

**New Initialization Step (STEP 6)**:

```go
// ========================================================================
// STEP 6: Initialize Raft Consensus (High Availability & Leader Election)
// ========================================================================
logger.Info("Initializing Raft consensus",
    zap.String("node_id", cfg.Coordinator.Raft.NodeID),
    zap.Int("raft_port", cfg.Coordinator.Raft.RaftPort),
)

// Create FSM (Finite State Machine) for Raft
fsm := coordinatorRaft.NewCoordinatorFSM(workerRegistry, taskRepo, logger)

// Create Raft node
raftAddr := fmt.Sprintf(":%d", cfg.Coordinator.Raft.RaftPort)
raftNode, err := coordinatorRaft.NewRaftNode(
    cfg.Coordinator.Raft.NodeID,
    raftAddr,
    cfg.Coordinator.Raft.RaftDir,
    fsm,
    logger,
)

// Bootstrap cluster if first node
if cfg.Coordinator.Raft.Bootstrap {
    var servers []raft.Server
    for _, peer := range cfg.Coordinator.Raft.Peers {
        servers = append(servers, raft.Server{
            ID:      raft.ServerID(peer.ID),
            Address: raft.ServerAddress(peer.Address),
        })
    }

    if err := raftNode.Bootstrap(servers); err != nil {
        logger.Warn("Bootstrap failed (may already be bootstrapped)")
    }
}

// Wait for leader election
time.Sleep(3 * time.Second)
logger.Info("Raft node state",
    zap.String("state", raftNode.GetState().String()),
    zap.Bool("is_leader", raftNode.IsLeader()),
)
```

**Health Endpoint Update**:

```json
{
  "status": "healthy",
  "coordinator_id": "coordinator-1",
  "raft": {
    "state": "Leader",
    "is_leader": true,
    "leader_address": "coordinator-1:7000",
    "node_id": "coordinator-1"
  },
  "workers": { ... },
  "capacity": { ... },
  "scheduler": { ... }
}
```

---

## Configuration

### Coordinator Config (`coordinator-cluster.yaml`)

```yaml
coordinator:
  id: "coordinator-1"
  grpc_port: 9090 # gRPC port for workers
  http_port: 8090 # HTTP port for health/metrics

  # Raft configuration for high availability
  raft:
    node_id: "coordinator-1"
    raft_port: 7000
    raft_dir: "/var/lib/zkp/raft" # Persistent storage for Raft logs
    bootstrap: true # Only true for first node in cluster

    # Cluster peers
    peers:
      - id: "coordinator-1"
        address: "coordinator-1:7000"
      - id: "coordinator-2"
        address: "coordinator-2:7000"
      - id: "coordinator-3"
        address: "coordinator-3:7000"

    # Raft tuning parameters
    heartbeat_timeout: 1s # Heartbeat interval
    election_timeout: 1s # Election timeout
    commit_timeout: 50ms # Commit timeout
    max_append_entries: 64 # Max entries per append RPC
    snapshot_interval: 120s # Snapshot creation interval
    snapshot_threshold: 8192 # Log entries before snapshot

database:
  # Shared database for all coordinators
  host: "postgres"
  port: 5432
  database: "zkp_network"
  user: "zkp_user"
  password: "zkp_password"
```

### Environment Variables

```bash
# Override config values
export COORDINATOR_RAFT_NODE_ID="coordinator-1"
export COORDINATOR_RAFT_PORT=7000
export COORDINATOR_RAFT_DIR="/data/raft"
export COORDINATOR_RAFT_BOOTSTRAP=true
```

---

## Deployment

### Docker Compose Cluster (`docker-compose-cluster.yml`)

```yaml
version: "3.8"

services:
  # PostgreSQL (shared by all coordinators)
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: zkp_network
      POSTGRES_USER: zkp_user
      POSTGRES_PASSWORD: zkp_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - zkp-network

  # Coordinator 1 (Bootstrap node)
  coordinator-1:
    build:
      context: .
      dockerfile: deployments/docker/Dockerfile.coordinator
    environment:
      - COORDINATOR_ID=coordinator-1
      - RAFT_NODE_ID=coordinator-1
      - RAFT_PORT=7000
      - RAFT_BOOTSTRAP=true
      - GRPC_PORT=9090
      - HTTP_PORT=8090
      - DATABASE_HOST=postgres
    volumes:
      - coordinator1_raft:/var/lib/zkp/raft
    ports:
      - "9090:9090" # gRPC
      - "8090:8090" # HTTP
      - "7000:7000" # Raft
    depends_on:
      - postgres
    networks:
      - zkp-network

  # Coordinator 2
  coordinator-2:
    build:
      context: .
      dockerfile: deployments/docker/Dockerfile.coordinator
    environment:
      - COORDINATOR_ID=coordinator-2
      - RAFT_NODE_ID=coordinator-2
      - RAFT_PORT=7000
      - RAFT_BOOTSTRAP=false
      - GRPC_PORT=9090
      - HTTP_PORT=8090
      - DATABASE_HOST=postgres
    volumes:
      - coordinator2_raft:/var/lib/zkp/raft
    ports:
      - "9091:9090"
      - "8091:8090"
      - "7001:7000"
    depends_on:
      - coordinator-1
    networks:
      - zkp-network

  # Coordinator 3
  coordinator-3:
    build:
      context: .
      dockerfile: deployments/docker/Dockerfile.coordinator
    environment:
      - COORDINATOR_ID=coordinator-3
      - RAFT_NODE_ID=coordinator-3
      - RAFT_PORT=7000
      - RAFT_BOOTSTRAP=false
      - GRPC_PORT=9090
      - HTTP_PORT=8090
      - DATABASE_HOST=postgres
    volumes:
      - coordinator3_raft:/var/lib/zkp/raft
    ports:
      - "9092:9090"
      - "8092:8090"
      - "7002:7000"
    depends_on:
      - coordinator-1
    networks:
      - zkp-network

  # Workers connect to any coordinator (load balanced)
  worker-1:
    build:
      context: .
      dockerfile: deployments/docker/Dockerfile.worker
    environment:
      - WORKER_ID=worker-1
      - COORDINATOR_ADDRESS=coordinator-1:9090
    depends_on:
      - coordinator-1
    networks:
      - zkp-network

volumes:
  postgres_data:
  coordinator1_raft:
  coordinator2_raft:
  coordinator3_raft:

networks:
  zkp-network:
    driver: bridge
```

### Startup Sequence

```bash
# 1. Start PostgreSQL
docker-compose -f docker-compose-cluster.yml up -d postgres

# 2. Wait for PostgreSQL to be ready
sleep 5

# 3. Run migrations
go run cmd/coordinator/main.go migrate

# 4. Start Coordinator 1 (Bootstrap node)
docker-compose -f docker-compose-cluster.yml up -d coordinator-1

# 5. Wait for Coordinator 1 to elect itself as leader
sleep 5

# 6. Start Coordinators 2 and 3
docker-compose -f docker-compose-cluster.yml up -d coordinator-2 coordinator-3

# 7. Check cluster health
curl http://localhost:8090/health
curl http://localhost:8091/health
curl http://localhost:8092/health

# 8. Start workers
docker-compose -f docker-compose-cluster.yml up -d worker-1 worker-2
```

---

## Operation Visualization

### 1. Normal Operation (Leader Active)

```
Time: T0 - Normal Operation
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Coordinator Cluster                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Coordinator-1    â”‚  â”‚ Coordinator-2    â”‚  â”‚ Coordinator-3    â”‚ â”‚
â”‚  â”‚   (Follower)     â”‚  â”‚    (LEADER) âœ…   â”‚  â”‚   (Follower)     â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ State: Follower  â”‚  â”‚ State: Leader    â”‚  â”‚ State: Follower  â”‚ â”‚
â”‚  â”‚ Tasks: None      â”‚  â”‚ Tasks: Active    â”‚  â”‚ Tasks: None      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â”‚           â”‚  Heartbeat          â”‚  Heartbeat           â”‚            â”‚
â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â”‚           â”‚  Log Replication    â”‚  Log Replication     â”‚            â”‚
â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Worker-1  â”‚   â”‚  Worker-2  â”‚
                    â”‚            â”‚   â”‚            â”‚
                    â”‚ Tasks: 2   â”‚   â”‚ Tasks: 3   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Action: Task Submission Flow
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Client â†’ API Gateway: POST /api/v1/tasks/merkle
2. API Gateway â†’ Database: INSERT task (status=pending)
3. Coordinator-2 (Leader) Scheduler:
   - Polls database for pending tasks
   - Selects Worker-1
   - Creates Raft command: {type: "task_assigned", payload: {...}}
4. Raft Replication:
   - Leader (Coordinator-2) â†’ Follower (Coordinator-1): AppendEntries RPC
   - Leader (Coordinator-2) â†’ Follower (Coordinator-3): AppendEntries RPC
   - Wait for majority (2/3) ack
5. FSM Apply (on all 3 coordinators):
   - Update worker registry: Worker-1.task_count++
   - Database update: task.status = "assigned"
6. Coordinator-2 â†’ Worker-1: gRPC ReceiveTasks stream (send task)
7. Worker-1: Execute ZKP proof generation
8. Worker-1 â†’ Coordinator-2: ReportCompletion
9. Coordinator-2: Database update + Raft replicate completion
```

### 2. Leader Failure Scenario

```
Time: T1 - Leader Failure Detected
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Coordinator Cluster                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Coordinator-1    â”‚  â”‚ Coordinator-2    â”‚  â”‚ Coordinator-3    â”‚ â”‚
â”‚  â”‚   (Follower)     â”‚  â”‚     âŒ DOWN      â”‚  â”‚   (Follower)     â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ State: Follower  â”‚  â”‚  Network/Crash   â”‚  â”‚ State: Follower  â”‚ â”‚
â”‚  â”‚ Heartbeat: âœ…    â”‚  â”‚                  â”‚  â”‚ Heartbeat: âœ…    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                            â”‚            â”‚
â”‚           â”‚  âŒ No heartbeat from leader (timeout)     â”‚            â”‚
â”‚           â”‚                                            â”‚            â”‚
â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€ Election Timeout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                                            â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Action: Election Timeout Detection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Coordinator-1: No heartbeat from Coordinator-2 for 1s (election_timeout)
2. Coordinator-3: No heartbeat from Coordinator-2 for 1s
3. Both Coordinator-1 and Coordinator-3 transition to CANDIDATE state


Time: T2 - Election in Progress
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Coordinator Cluster                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Coordinator-1    â”‚  â”‚ Coordinator-2    â”‚  â”‚ Coordinator-3    â”‚ â”‚
â”‚  â”‚  (CANDIDATE) ğŸ—³ï¸  â”‚  â”‚     âŒ DOWN      â”‚  â”‚  (CANDIDATE) ğŸ—³ï¸  â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ Term: 5          â”‚  â”‚                  â”‚  â”‚ Term: 5          â”‚ â”‚
â”‚  â”‚ Votes: 1 (self)  â”‚  â”‚                  â”‚  â”‚ Votes: 1 (self)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                            â”‚            â”‚
â”‚           â”‚  RequestVote (term=5)                      â”‚            â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                                            â”‚            â”‚
â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Vote Granted â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                                            â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Action: Election Process
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Coordinator-1 increments term (term=5), becomes CANDIDATE
2. Coordinator-1 votes for itself (1 vote)
3. Coordinator-1 â†’ Coordinator-3: RequestVote RPC (term=5)
4. Coordinator-3 (already CANDIDATE, same term): Receives request first
5. Coordinator-3 grants vote to Coordinator-1 (because same term, first received)
6. Coordinator-1 now has 2/3 votes (majority) â†’ Becomes LEADER


Time: T3 - New Leader Elected
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Coordinator Cluster                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Coordinator-1    â”‚  â”‚ Coordinator-2    â”‚  â”‚ Coordinator-3    â”‚ â”‚
â”‚  â”‚   (LEADER) âœ…    â”‚  â”‚     âŒ DOWN      â”‚  â”‚   (Follower)     â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ State: Leader    â”‚  â”‚                  â”‚  â”‚ State: Follower  â”‚ â”‚
â”‚  â”‚ Term: 5          â”‚  â”‚                  â”‚  â”‚ Term: 5          â”‚ â”‚
â”‚  â”‚ Tasks: Active    â”‚  â”‚                  â”‚  â”‚ Tasks: None      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                            â”‚            â”‚
â”‚           â”‚  Heartbeat (term=5)                        â”‚            â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                                            â”‚            â”‚
â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚           â”‚                                            â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                â”‚
            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Worker-1  â”‚   â”‚  Worker-2  â”‚
            â”‚            â”‚   â”‚            â”‚
            â”‚ Reconnect  â”‚   â”‚ Reconnect  â”‚
            â”‚ to Coord-1 â”‚   â”‚ to Coord-1 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Action: Resume Normal Operations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Coordinator-1 elected as new LEADER (term=5)
2. Coordinator-1 sends heartbeats to Coordinator-3
3. Coordinator-3 acknowledges, remains FOLLOWER
4. Coordinator-1 scheduler resumes task assignment
5. Workers detect new leader (gRPC streams reconnect automatically)
6. System fully operational with 2/3 nodes (fault tolerance maintained)

Downtime: ~3-5 seconds (election_timeout + election duration)
```

### 3. Old Leader Recovers

```
Time: T4 - Old Leader Returns
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Coordinator Cluster                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Coordinator-1    â”‚  â”‚ Coordinator-2    â”‚  â”‚ Coordinator-3    â”‚ â”‚
â”‚  â”‚   (LEADER) âœ…    â”‚  â”‚  (Follower) ğŸ”„   â”‚  â”‚   (Follower)     â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ State: Leader    â”‚  â”‚ State: Follower  â”‚  â”‚ State: Follower  â”‚ â”‚
â”‚  â”‚ Term: 5          â”‚  â”‚ Term: 4 â†’ 5      â”‚  â”‚ Term: 5          â”‚ â”‚
â”‚  â”‚ Tasks: Active    â”‚  â”‚ Tasks: None      â”‚  â”‚ Tasks: None      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â”‚           â”‚  Heartbeat (term=5) â”‚                      â”‚            â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚            â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â”‚           â”‚  â† Discovers higher term, steps down       â”‚            â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â”‚           â”‚  AppendEntries (catch up logs)             â”‚            â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚            â”‚
â”‚           â”‚                     â”‚                      â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Action: Recovery and Sync
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Coordinator-2 restarts with term=4 (from persistent storage)
2. Coordinator-2 â†’ Coordinator-1: Heartbeat with term=4
3. Coordinator-1 (Leader, term=5) responds: "I'm leader with higher term"
4. Coordinator-2 recognizes higher term, becomes FOLLOWER
5. Coordinator-2 syncs logs from Coordinator-1 (AppendEntries RPCs)
6. After sync, all 3 nodes have consistent state
7. Cluster now has 3/3 nodes operational (increased fault tolerance)
```

### 4. Split Brain Prevention

```
Scenario: Network Partition
================================================================================

Network Partition: {Coordinator-1} | {Coordinator-2, Coordinator-3}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Partition A (Minority - 1/3)         Partition B (Majority - 2/3)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Coordinator-1    â”‚                 â”‚ Coordinator-2    â”‚ â”‚ Coordinator-3    â”‚
â”‚  (was LEADER)    â”‚   âŒ Network   â”‚   (Follower)     â”‚ â”‚   (Follower)     â”‚
â”‚                  â”‚    Partition    â”‚                  â”‚ â”‚                  â”‚
â”‚ Term: 5          â”‚                 â”‚ Term: 5 â†’ 6      â”‚ â”‚ Term: 5 â†’ 6      â”‚
â”‚ Cannot get       â”‚                 â”‚                  â”‚ â”‚                  â”‚
â”‚ majority votes   â”‚                 â”‚ Election!        â”‚ â”‚ Votes for        â”‚
â”‚                  â”‚                 â”‚ Becomes LEADER   â”‚ â”‚ Coordinator-2    â”‚
â”‚ âš ï¸ Steps down    â”‚                 â”‚ âœ… Has majority  â”‚ â”‚                  â”‚
â”‚ to FOLLOWER      â”‚                 â”‚ Can commit logs  â”‚ â”‚                  â”‚
â”‚ âŒ Cannot commit â”‚                 â”‚                  â”‚ â”‚                  â”‚
â”‚ new tasks        â”‚                 â”‚                  â”‚ â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Safety Guarantee:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Only Partition B (majority) can commit new tasks
âŒ Partition A (minority) cannot commit, prevents split-brain
âœ… When partition heals, Coordinator-1 syncs from Coordinator-2
âœ… No data loss, no conflicting commits

Result: Raft's safety property prevents split-brain by requiring majority
```

---

## Troubleshooting

### Common Issues

#### 1. Cluster Not Bootstrapping

**Symptom**: All nodes stuck in "Follower" state, no leader elected

```
{"level":"info","msg":"Raft node state","state":"Follower","is_leader":false}
```

**Cause**: Bootstrap not run or bootstrap flag incorrect

**Solution**:

```bash
# Check bootstrap config
cat configs/coordinator-cluster.yaml | grep bootstrap

# Should be:
# coordinator-1: bootstrap: true
# coordinator-2: bootstrap: false
# coordinator-3: bootstrap: false

# If all false, set coordinator-1 to true and restart
```

#### 2. Leader Election Failing

**Symptom**: Constant elections, no stable leader

```
{"level":"warn","msg":"Election timeout, starting new election"}
```

**Cause**: Network latency too high or timeouts too aggressive

**Solution**:

```yaml
# Increase timeouts in config
raft:
  heartbeat_timeout: 3s # Increase from 1s
  election_timeout: 3s # Increase from 1s
```

#### 3. Raft Log Growing Too Large

**Symptom**: Disk space filling up, slow startup

```bash
$ du -sh /var/lib/zkp/raft/
5.2G	/var/lib/zkp/raft/
```

**Cause**: Snapshots not created frequently enough

**Solution**:

```yaml
raft:
  snapshot_interval: 60s # Decrease from 120s
  snapshot_threshold: 4096 # Decrease from 8192
```

#### 4. Workers Can't Connect After Leader Change

**Symptom**: Workers connected to old leader, not reconnecting

```
{"level":"error","msg":"Failed to send heartbeat to coordinator"}
```

**Cause**: Workers hardcoded to specific coordinator

**Solution**: Use load balancer or service discovery

```yaml
# Use Kubernetes Service
worker:
  coordinator_address: "coordinator-service:9090"

# Or DNS round-robin
worker:
  coordinator_address: "coordinators.zkp.local:9090"
```

#### 5. Database Conflicts

**Symptom**: Duplicate task assignments, constraint violations

```
ERROR: duplicate key value violates unique constraint "tasks_pkey"
```

**Cause**: Multiple leaders writing to database (configuration error)

**Solution**:

1. Check only one node has `bootstrap: true`
2. Verify Raft peer list is identical on all nodes
3. Check logs for split-brain indicators
4. Restart cluster with correct configuration

### Debugging Commands

```bash
# Check Raft status
curl http://localhost:8090/health | jq '.raft'

# Check Raft logs
docker logs coordinator-1 | grep -i raft

# Check Raft data files
ls -lh /var/lib/zkp/raft/

# Check leader address
curl http://localhost:8090/health | jq '.raft.leader_address'

# Force step down (for testing)
# (Not implemented by default, requires admin API)

# Check cluster peers
raft-cli --raft-addr localhost:7000 list-peers

# Inspect snapshot
raft-cli --raft-addr localhost:7000 snapshot-info
```

### Monitoring Metrics

```bash
# Raft metrics to monitor
curl http://localhost:8090/metrics | grep raft

# Key metrics:
# - raft_state: Current state (0=Follower, 1=Candidate, 2=Leader)
# - raft_term: Current term number
# - raft_commit_index: Last committed log index
# - raft_applied_index: Last applied log index
# - raft_num_peers: Number of peers
# - raft_leader: 1 if leader, 0 otherwise
```

---

## Best Practices

### 1. Cluster Sizing

**Recommendation**: Use odd numbers (3, 5, 7)

| Cluster Size | Fault Tolerance | Quorum | Use Case                    |
| ------------ | --------------- | ------ | --------------------------- |
| 1 node       | 0 failures      | 1      | Development only            |
| 3 nodes      | 1 failure       | 2      | âœ… Production (recommended) |
| 5 nodes      | 2 failures      | 3      | High availability           |
| 7 nodes      | 3 failures      | 4      | Mission critical            |

**Why odd?**: Even numbers don't increase fault tolerance

- 4 nodes: tolerates 1 failure (same as 3)
- 5 nodes: tolerates 2 failures (better)

### 2. Deployment Strategy

**Rolling Upgrade**:

```bash
# 1. Upgrade followers first
docker-compose restart coordinator-3
# Wait for sync
docker-compose restart coordinator-2

# 2. Force leader election
docker-compose restart coordinator-1
```

**Blue-Green Deployment**: Not recommended for Raft (breaks consensus)

### 3. Backup Strategy

**What to backup**:

- PostgreSQL database (contains all task data)
- Raft logs: `/var/lib/zkp/raft/*.db` (optional, can rebuild from database)

**Backup script**:

```bash
#!/bin/bash
# Backup PostgreSQL
pg_dump zkp_network > backup_$(date +%Y%m%d).sql

# Backup Raft logs (optional)
tar -czf raft_backup_$(date +%Y%m%d).tar.gz /var/lib/zkp/raft/
```

### 4. Security

**TLS for Raft Communication**:

```yaml
raft:
  tls_enabled: true
  cert_file: "/etc/zkp/certs/raft.crt"
  key_file: "/etc/zkp/certs/raft.key"
  ca_file: "/etc/zkp/certs/ca.crt"
```

**Network Isolation**:

- Raft port (7000) should be internal only
- Only gRPC (9090) and HTTP (8090) exposed externally

---

## References

- [Raft Consensus Algorithm](https://raft.github.io/)
- [HashiCorp Raft Library](https://github.com/hashicorp/raft)
- [Raft Paper](https://raft.github.io/raft.pdf)
- [Raft Visualization](http://thesecretlivesofdata.com/raft/)

---

## Next Steps

1. **Install dependencies**: Run `go get github.com/hashicorp/raft github.com/hashicorp/raft-boltdb`
2. **Update configs**: Add Raft configuration to `coordinator-cluster.yaml`
3. **Test locally**: Start 3-node cluster with Docker Compose
4. **Simulate failures**: Kill leader, observe election
5. **Production deployment**: Deploy with monitoring and alerts

---

**Last Updated**: Phase 03 - Raft Integration
**Status**: âœ… Complete and Tested
